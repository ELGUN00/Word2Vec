{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x289bb148a70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2673, -0.4212, -0.5107, -1.5727, -0.1232]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"salam\": 0, \"necəsən\": 1}\n",
    "embeds = nn.Embedding(2, 5)  \n",
    "lookup_tensor = torch.tensor([word_to_ix[\"necəsən\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[516.8106060028076, 514.3368418216705, 511.87966799736023, 509.4371349811554, 507.0097324848175, 504.5950999259949, 502.19280886650085, 499.8031039237976, 497.42474818229675, 495.05787801742554]\n",
      "tensor([-0.2692,  1.3117, -0.8906, -0.7521, -0.0578,  2.2006, -0.7550, -0.4346,\n",
      "         0.6704,  0.6505], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "      \n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "     \n",
    "        model.zero_grad()\n",
    "\n",
    "      \n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "       \n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  \n",
    "\n",
    "\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  \n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    s =s.lower().replace('ə','e')\n",
    "    s =s.replace('ı','i')\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gunogeegqes\n"
     ]
    }
   ],
   "source": [
    "print(unicodeToAscii('günöğəəğqeş'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "CONTEXT_SIZE = 2 \n",
    "EMDEDDING_DIM = 100\n",
    "\n",
    "raw_text = \"\"\"Mənim halım dəyişdi, ayaqlarım taqətdən düşdü. \n",
    "Parçalanmış paroxodda bu quldur dəstəsilə qalmaq dəhşət idi. \n",
    "Ancaq durub gözləmək, fikirləşmək vaxtı deyildi. İndi bu qayıq \n",
    "bizim özümüzə lazım idi, odur ki, biz hər nə cür olursa olsun \n",
    "həmin qayığı tapmalı idik. Biz paroxodun sağ tərəfi ilə getməyə \n",
    "başladıq, titrəyib əsə-əsə, güc-bəla ilə dal tərəfə gəlib çatdıq; \n",
    "mənə elə gəlirdi ki, buraya çatıncayadək azı bir həftə keçdi. \n",
    "Qayıqdan heç bir nişanə yox idi. Cim dedi ki, o deyəsən daha gedə \n",
    "bilməyəcək, qorxudan lap taqətini itirib, tamam gücdən düşüb. \n",
    "Mən ona bildirdim ki, necə olursa olsun getmək lazımdır, əgər \n",
    "burada qalsaq işimiz pis olacaq, bu lap yəqin m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Biz yollandıq. Kapitan kayutunun dal tərəfini axtarmağa başladıq, \n",
    "tapdıq, sonra gücümüzü toplayıb qaranlıqda, gözəyarı paroxodun \n",
    "kənarlarındakı çıxıntılardan tuta-tuta irəlilədik, çünki onun bir \n",
    "tərəfı artıq suyun içində idi, beləliklə, bil losman budkasının \n",
    "şüşəbəndinə çatdıq. Qapıya çatan kimi gördük ki, qayıq buradadır. \n",
    "Mən onu qaranlıqda güclə görmüşdüm. Sevindiyimdən bilmirdim nə edim! \n",
    "Bir saniyə də keçsəydi mən onun içində olacaqdım, ancaq bu vaxt qapı açıldı.\"\"\".split()\n",
    "\n",
    "for ix,word in enumerate(raw_text):\n",
    "    raw_text[ix] = unicodeToAscii(word)\n",
    "    \n",
    "vocab = set(raw_text)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['halim', 'deyisdi,', 'taqetden', 'dusdu.'], 'ayaqlarim')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "      \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "       \n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, EMDEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)  \n",
    "\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: olursa hemin tapmali idik. taqetden dusdu. parcalanmis paroxodda bu quldur destesile qalmaq dehset idi. ancaq durub gozlemek, fikirlesmek vaxti deyildi. indi bu qayiq bizim ozumuze lazim idi, odur ki, biz her ne cur olursa olsun hemin qayigi tapmali idik. biz paroxodun sag terefi ile getmeye basladiq, titreyib eseese, gucbela ile dal terefe gelib catdiq; mene ele gelirdi ki, buraya catincayadek azi bir hefte kecdi. qayiqdan hec bir nisane yox idi. cim dedi ki, o deyesen daha gede bilmeyecek, qorxudan lap taqetini itirib, tamam gucden dusub. men ona bildirdim ki, nece olursa olsun getmek lazimdir, eger burada qalsaq isimiz pis olacaq, bu lap yeqin meseledir. biz yollandiq. kapitan kayutunun dal terefini axtarmaga basladiq, tapdiq, sonra gucumuzu toplayib qaranliqda, gozeyari paroxodun kenarlarindaki cixintilardan tutatuta ireliledik, cunki onun bir terefi artiq suyun icinde idi, belelikle, bil losman budkasinin susebendine catdiq. qapiya catan kimi gorduk ki, qayiq buradadir. men onu qaranliqda gucle gormusdum. sevindiyimden bilmirdim ne edim bir saniye de kecseydi men onun icinde olacaqdim, ancaq bu vaxt qapi acildi.\n",
      "\n",
      "Context: ['men', 'ona', 'ki,', 'nece']\n",
      "\n",
      "Prediction: bildirdim\n"
     ]
    }
   ],
   "source": [
    "context = ['men','ona','ki,', 'nece']\n",
    "for ix,word in enumerate(context):\n",
    "    context[ix] = unicodeToAscii(word)\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: olursa hemin tapmali idik. taqetden dusdu. parcalanmis paroxodda bu quldur destesile qalmaq dehset idi. ancaq durub gozlemek, fikirlesmek vaxti deyildi. indi bu qayiq bizim ozumuze lazim idi, odur ki, biz her ne cur olursa olsun hemin qayigi tapmali idik. biz paroxodun sag terefi ile getmeye basladiq, titreyib eseese, gucbela ile dal terefe gelib catdiq; mene ele gelirdi ki, buraya catincayadek azi bir hefte kecdi. qayiqdan hec bir nisane yox idi. cim dedi ki, o deyesen daha gede bilmeyecek, qorxudan lap taqetini itirib, tamam gucden dusub. men ona bildirdim ki, nece olursa olsun getmek lazimdir, eger burada qalsaq isimiz pis olacaq, bu lap yeqin meseledir. biz yollandiq. kapitan kayutunun dal terefini axtarmaga basladiq, tapdiq, sonra gucumuzu toplayib qaranliqda, gozeyari paroxodun kenarlarindaki cixintilardan tutatuta ireliledik, cunki onun bir terefi artiq suyun icinde idi, belelikle, bil losman budkasinin susebendine catdiq. qapiya catan kimi gorduk ki, qayiq buradadir. men onu qaranliqda gucle gormusdum. sevindiyimden bilmirdim ne edim bir saniye de kecseydi men onun icinde olacaqdim, ancaq bu vaxt qapi acildi.\n",
      "\n",
      "Context: ['olursa', 'hemin', 'tapmali', 'idik.']\n",
      "\n",
      "Prediction: qayigi\n"
     ]
    }
   ],
   "source": [
    "context = ['olursa','həmin','tapmali', 'idik.']\n",
    "for ix,word in enumerate(context):\n",
    "    context[ix] = unicodeToAscii(word)\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: olursa hemin tapmali idik. taqetden dusdu. parcalanmis paroxodda bu quldur destesile qalmaq dehset idi. ancaq durub gozlemek, fikirlesmek vaxti deyildi. indi bu qayiq bizim ozumuze lazim idi, odur ki, biz her ne cur olursa olsun hemin qayigi tapmali idik. biz paroxodun sag terefi ile getmeye basladiq, titreyib eseese, gucbela ile dal terefe gelib catdiq; mene ele gelirdi ki, buraya catincayadek azi bir hefte kecdi. qayiqdan hec bir nisane yox idi. cim dedi ki, o deyesen daha gede bilmeyecek, qorxudan lap taqetini itirib, tamam gucden dusub. men ona bildirdim ki, nece olursa olsun getmek lazimdir, eger burada qalsaq isimiz pis olacaq, bu lap yeqin meseledir. biz yollandiq. kapitan kayutunun dal terefini axtarmaga basladiq, tapdiq, sonra gucumuzu toplayib qaranliqda, gozeyari paroxodun kenarlarindaki cixintilardan tutatuta ireliledik, cunki onun bir terefi artiq suyun icinde idi, belelikle, bil losman budkasinin susebendine catdiq. qapiya catan kimi gorduk ki, qayiq buradadir. men onu qaranliqda gucle gormusdum. sevindiyimden bilmirdim ne edim bir saniye de kecseydi men onun icinde olacaqdim, ancaq bu vaxt qapi acildi.\n",
      "\n",
      "Context: ['bu', 'quldur', 'tapmali', 'idik.']\n",
      "\n",
      "Prediction: biz\n"
     ]
    }
   ],
   "source": [
    "context = ['bu','quldur','tapmali', 'idik.']\n",
    "for ix,word in enumerate(context):\n",
    "    context[ix] = unicodeToAscii(word)\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
