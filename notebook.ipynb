{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d2ee648b10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"salam\": 0, \"necəsən\": 1}\n",
    "embeds = nn.Embedding(2, 5)  \n",
    "lookup_tensor = torch.tensor([word_to_ix[\"necəsən\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix['winters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[523.4939029216766, 521.1120574474335, 518.748562335968, 516.4032638072968, 514.0753750801086, 511.7617199420929, 509.4619884490967, 507.1765878200531, 504.9051568508148, 502.64693689346313]\n",
      "tensor([ 0.0470,  0.6477,  0.1442,  0.1578, -0.9045,  0.8705,  0.9191,  1.1073,\n",
      "         2.1022, -0.1647], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "      \n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "     \n",
    "        model.zero_grad()\n",
    "\n",
    "      \n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "       \n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  \n",
    "\n",
    "\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix['cold.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'cold.'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    # dist = torch.norm( - model.embeddings.weight[word_to_ix[w]])\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>small</td>\n",
       "      <td>0.005815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>thine</td>\n",
       "      <td>0.006417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>If</td>\n",
       "      <td>0.007980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>eyes,</td>\n",
       "      <td>0.017048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>forty</td>\n",
       "      <td>0.020096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>When</td>\n",
       "      <td>0.030605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>couldst</td>\n",
       "      <td>0.031955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>it</td>\n",
       "      <td>0.037172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>held:</td>\n",
       "      <td>0.037192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>art</td>\n",
       "      <td>0.037457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  similarity\n",
       "70    small    0.005815\n",
       "52    thine    0.006417\n",
       "42       If    0.007980\n",
       "20    eyes,    0.017048\n",
       "56    forty    0.020096\n",
       "35     When    0.030605\n",
       "64  couldst    0.031955\n",
       "18       it    0.037172\n",
       "0     held:    0.037192\n",
       "13      art    0.037457"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4183, -1.1325,  1.3679, -0.1839,  0.2949,  0.3500,  0.2918,  2.5289,\n",
       "        -2.2841, -1.2369], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight[word_to_ix[\"When\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  \n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    s =s.lower().replace('ə','e')\n",
    "    s =s.replace('ı','i')\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gunogeegqes\n"
     ]
    }
   ],
   "source": [
    "print(unicodeToAscii('günöğəəğqeş'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "CONTEXT_SIZE = 2 \n",
    "EMDEDDING_DIM = 100\n",
    "file = open('nagil.txt','r',encoding=\"utf8\").read()\n",
    "\n",
    "raw_text = file.split()\n",
    "\n",
    "\n",
    "for ix,word in enumerate(raw_text):\n",
    "    raw_text[ix] = unicodeToAscii(word)\n",
    "    \n",
    "vocab = set(raw_text)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['texnologiyalari', 'insanlarin', 'suretle', 'daxil'], 'heyatina')"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "      \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "       \n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, EMDEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total loss: 18115.966796875\n",
      "epoch: 1 total loss: 17146.6015625\n",
      "epoch: 2 total loss: 16413.990234375\n",
      "epoch: 3 total loss: 16028.76953125\n",
      "epoch: 4 total loss: 15451.2177734375\n",
      "epoch: 5 total loss: 15487.77734375\n",
      "epoch: 6 total loss: 14898.203125\n",
      "epoch: 7 total loss: 14855.1162109375\n",
      "epoch: 8 total loss: 14320.1552734375\n",
      "epoch: 9 total loss: 14240.9375\n",
      "epoch: 10 total loss: 13675.439453125\n",
      "epoch: 11 total loss: 13644.5537109375\n",
      "epoch: 12 total loss: 14299.654296875\n",
      "epoch: 13 total loss: 13720.18359375\n",
      "epoch: 14 total loss: 12671.6103515625\n",
      "epoch: 15 total loss: 12051.185546875\n",
      "epoch: 16 total loss: 12234.6552734375\n",
      "epoch: 17 total loss: 11180.5029296875\n",
      "epoch: 18 total loss: 10111.392578125\n",
      "epoch: 19 total loss: 10763.447265625\n",
      "epoch: 20 total loss: 10269.3466796875\n",
      "epoch: 21 total loss: 10822.2880859375\n",
      "epoch: 22 total loss: 10101.205078125\n",
      "epoch: 23 total loss: 8901.2158203125\n",
      "epoch: 24 total loss: 8685.5654296875\n",
      "epoch: 25 total loss: 8386.7333984375\n",
      "epoch: 26 total loss: 7030.10009765625\n",
      "epoch: 27 total loss: 7156.908203125\n",
      "epoch: 28 total loss: 8926.78125\n",
      "epoch: 29 total loss: 7987.337890625\n",
      "epoch: 30 total loss: 6196.673828125\n",
      "epoch: 31 total loss: 5436.61865234375\n",
      "epoch: 32 total loss: 4767.62841796875\n",
      "epoch: 33 total loss: 3829.36181640625\n",
      "epoch: 34 total loss: 3946.99462890625\n",
      "epoch: 35 total loss: 7726.203125\n",
      "epoch: 36 total loss: 7976.24462890625\n",
      "epoch: 37 total loss: 8124.15966796875\n",
      "epoch: 38 total loss: 6498.28662109375\n",
      "epoch: 39 total loss: 5430.3193359375\n",
      "epoch: 40 total loss: 4464.45751953125\n",
      "epoch: 41 total loss: 3807.097412109375\n",
      "epoch: 42 total loss: 3232.858154296875\n",
      "epoch: 43 total loss: 2631.350830078125\n",
      "epoch: 44 total loss: 2087.72607421875\n",
      "epoch: 45 total loss: 2222.783447265625\n",
      "epoch: 46 total loss: 1738.822021484375\n",
      "epoch: 47 total loss: 2511.6640625\n",
      "epoch: 48 total loss: 1676.4931640625\n",
      "epoch: 49 total loss: 1522.7156982421875\n",
      "epoch: 50 total loss: 1017.3003540039062\n",
      "epoch: 51 total loss: 1033.597900390625\n",
      "epoch: 52 total loss: 806.99658203125\n",
      "epoch: 53 total loss: 770.78173828125\n",
      "epoch: 54 total loss: 701.696533203125\n",
      "epoch: 55 total loss: 1151.1776123046875\n",
      "epoch: 56 total loss: 596.3683471679688\n",
      "epoch: 57 total loss: 373.951171875\n",
      "epoch: 58 total loss: 331.2070007324219\n",
      "epoch: 59 total loss: 299.9377746582031\n",
      "epoch: 60 total loss: 276.411865234375\n",
      "epoch: 61 total loss: 255.2581329345703\n",
      "epoch: 62 total loss: 237.65345764160156\n",
      "epoch: 63 total loss: 222.0775604248047\n",
      "epoch: 64 total loss: 208.8427734375\n",
      "epoch: 65 total loss: 197.4282989501953\n",
      "epoch: 66 total loss: 187.3963165283203\n",
      "epoch: 67 total loss: 178.42933654785156\n",
      "epoch: 68 total loss: 170.3530731201172\n",
      "epoch: 69 total loss: 163.03196716308594\n",
      "epoch: 70 total loss: 156.3612823486328\n",
      "epoch: 71 total loss: 150.2562255859375\n",
      "epoch: 72 total loss: 144.64410400390625\n",
      "epoch: 73 total loss: 139.46287536621094\n",
      "epoch: 74 total loss: 134.6651611328125\n",
      "epoch: 75 total loss: 130.20347595214844\n",
      "epoch: 76 total loss: 126.04994201660156\n",
      "epoch: 77 total loss: 122.16240692138672\n",
      "epoch: 78 total loss: 118.52062225341797\n",
      "epoch: 79 total loss: 115.1029052734375\n",
      "epoch: 80 total loss: 111.88446044921875\n",
      "epoch: 81 total loss: 108.85238647460938\n",
      "epoch: 82 total loss: 105.98808288574219\n",
      "epoch: 83 total loss: 103.28314208984375\n",
      "epoch: 84 total loss: 100.7236328125\n",
      "epoch: 85 total loss: 98.29822540283203\n",
      "epoch: 86 total loss: 95.99201202392578\n",
      "epoch: 87 total loss: 93.79541778564453\n",
      "epoch: 88 total loss: 91.70117950439453\n",
      "epoch: 89 total loss: 89.7021255493164\n",
      "epoch: 90 total loss: 87.79583740234375\n",
      "epoch: 91 total loss: 85.97440338134766\n",
      "epoch: 92 total loss: 84.23172760009766\n",
      "epoch: 93 total loss: 82.56167602539062\n",
      "epoch: 94 total loss: 80.9600830078125\n",
      "epoch: 95 total loss: 79.42424774169922\n",
      "epoch: 96 total loss: 77.94893646240234\n",
      "epoch: 97 total loss: 76.53170013427734\n",
      "epoch: 98 total loss: 75.16845703125\n",
      "epoch: 99 total loss: 73.85557556152344\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)  \n",
    "\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch: {epoch} total loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ki,', 'son', 'suni', 'intellekt'], 'zamanlar')"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: melumdur ki, son zamanlar suni intellekt texnologiyalari insanlarin heyatina suretle daxil olur. yeni texnologiyalar dunya olkelerinin siyasetine tesir edir, dovletlerin strateji potensialini mueyyenlesdirir. suni intellekt texnologiyalarindan hem herbi, hem de mulki meqsedlerle genis istifade olunur. artiq beseriyyetin inkisafinin bu merhelesinde suni intellekt insanlarin aile ve cemiyyet, hakimiyyet ve vetendas munasibetlerine guclu tesir ederek yeni realliqlar yaratmaqda, dovrumuzun ideologiyasina cevrilmekdedir. elbette ki, bu tendensiyanin da musbet ve menfi terefleri vardir. suni intellekt nezeriyyesinin elminezeri esaslarina geldikde, demek olar ki, insanlarin butun dovrlerde suni intellektle bagli arzulari olub. hetta bu texnologiyalar meydana gelmemisden once ayzek azimov, karel capek kimi fantast yazicilar oz eserlerinde suni intellekt meselelerine toxunub, muxtelif mulahizeler ireli surubler. lakin suni intellektin reallasmasi oten esrin ci illerinden sonra, komputer texnologiyalarinin ortaya cixmasi ile bas vermisdir. hemin dovrlerde komputerde hesablama ve mentiq emeliyyatlarinin heyata kecirilmesine nail olunurdusa, paralel olaraq alan turinq, lutfi zade kimi gorkemli alimleri bele bir sual dusundururdu komputerler, eyni zamanda insan kimi dusune, qerar qebul ede, yaxud gelecekle bagli mueyyen proqnozlar vere bilerlermi sohbet insana xas olan bezi intellektual funksiyalarin hemin o alqoritmlere, proqramlara, modellere kocurulmesinden gedirdi. qeyd etdiyim kimi, oten esrin ci illerinden etibaren bir sira alimler insan beynini teskil eden neyronlarin suni modellerini yaratmaga basladilar. lakin nezere almaq lazimdir ki, o dovrlerde komputerlerin ozunun imkanlari hele cox mehdud idi. ona gore de suni intellekt ideyalarinin heyata kecirilmesi, onun riyazifiziki esaslarinin inkisaf etdirilmesi ucun bir qeder zaman lazim idi. ci ilde a.turinqin cap etdirdiyi hesablama masinlari ve intellekt adli eser, eyni zamanda professor lutfi zadenin absin kolumbiya universitetinde cap olunmus suni intellekt nezeriyyesine aid meqalesi bu sahenin inkisafina ehemiyyetli tohfe olmusdur. suni intellekt terminini ise amerika alimi con makkarti ilk defe ci ilde dartmut kollecinin yay seminarinda teqdim etmisdir. bununla da, ozozune oyrenen, qavrayan ve yeni bilikler yaradan suni intellekt texnologiyalarinin esasi qoyulmusdur. iki meqami xususi qeyd etmek isteyirem. melumdur ki, tebii intellektin yaranmasini, generasiya olunmasini temin eden neyronlar birbiri ile muxtelif munasibetlerde, elaqelerde olmaqla, insanin bilik ortaya qoymasina getirib cixarir. bu gun suni intellektin esas dayaqlarindan biri beynin esasini teskil eden neyronlarin xususiyyetlerini oyrenmekdir. son dovrlerde derin telim texnologiyalari, muxtelif smart qurgular haqqinda cox esidirik. onlarin da esasinda suni neyron sebekeleri dayanir. bu, o demekdir ki, suni intellektin inkisaf etmesi mehz suni neyron ve onun sebekelerinin ortaya qoydugu mentiqiriyazi imkanlarla baglidir. suni intellekt onun esasinda qerar qebul edir, secir, taniyir, proqnozlar verir ve s. bu, birinci meqamdir. ikinci meqam ondan ibaretdir ki, insanin mentiqi yalniz  yaxud , he ve ya yox ikiqiymetli mentiq esasinda qurulmayibdir. he ve yox nisbidir  bu intervalda istenilen qiymet ola biler. bunun da riyazi ifadesini professor lutfi zade ci ilde qeyriselis coxluqlar ve qeyriselis mentiq nezeriyyesi ile vermisdir. bu nezeriyye suni intellektin inkisafinda inqilabi deyisikliye sebeb olmusdur. bundan basqa, suni intellekt biologiyadan yararlanan, qaynaqlanan bilikleri de exz edir. ayriayri canlilarin mueyyen rasional funksiyalari vardir ki, onlari modellesdirib bugunku ve gelecekde yarana bilecek problemlerin helli ucun istifade etmek mumkundur. tesadufi deyildir ki, bu gun genetik alqoritmler, qarisqa alqoritmi, ari surusunun alqoritmi ve s. ile qarsilasiriq. yeni alimler bioalemi oyrenmekle, oradaki davranisi, funksionalligi, imkanlari, munasibetleri askarlayib, riyazi modellerini qururlar ve alqoritmlesdirirler. onlarin da esasinda muxtelif intellektual qurgular, sistemler, proqram mehsullari yaradirlar. oten esrin ci illerinde movcud olan texnoloji potensial suni intellektle bagli ideyalari heyata kecirmeye imkan vermirdi. lakin komputerlerin kompleks imkanlari suret, yaddas, riyazi teminat ve s. artdiqca, evveller helli murekkeb olan meseleler bu gun superkomputerler vasitesile cox asanliqla hell olunur. diger terefden, muasir dovrde suni intellekt texnologiyalarina esaslanan proqram mehsullari  meselen, catbotlar movcuddur. bu proqramlar internetplatformada fealiyyet gosterir, ona bir kitabxana  neheng bilikler bazasi kimi baxilir. internetin butun resurslari onun ucun elcatandir. tesadufi deyildir ki, artiq olkeler, insan cemiyyeti bu kimi meselelere gore cox narahat olmaga baslayib. cunki geden prosesler cemiyyetin idare olunmasina menfi tesir ede biler.  siz suni intellekt esasinda hazirlanan texnologiyalardan danisdiniz. bes bu cur texnologiyalar, o cumleden robotlar gelecekde xaotik veziyyet yarada, insanligin eleyhine cevrile bilerlermi  melumdur ki, suni intellekt texnologiyalarinin yaradicisi insandir. bu kimi texnologiyalarin meydana gelmesinde fundamental elmler sahesinde fealiyyet gosteren fizikler, riyaziyyatcilar, kimyacilar illerle tedqiqatlar aparir, mueyyen neticeler elde edirler. yeni texnologiyalar, umumen, insanlarin, cemiyyetin xeyrine olsa da, onlarin riskleri qacilmazdir. duzdur, bezen olkeni xaricden olan tehlukelerden qorumaq, dovletin mudafie qudretini guclendirmek ucun muxtelif silahlar da yaradilir. bu, tebiidir. eyni zamanda insanlarin saglamligi ile bagli istehsal edilen ayriayri dermanlar, mualice usullari basqa meqsedlerle  bioloji, kimyevi silah kimi de istifade olunur. butun bunlar insanin iradesinden asili olan meselelerdir. tebii olaraq, sizin verdiyiniz sual ortaya cixir bes suni intellekt texnologiyalari hansisa merhelede insan iradesinden kenara cixa bilermi yeni suni intellektin imkanlari ele bir hedde cata biler ki, tebii intellekt onun qarsisinda aciz qalsin ve insan cemiyyetde dominantligini, yeni ali varliq olmasini itirsin melumdur ki, insan bioloji, suni intellekt ise texnoloji varliqdir ve ayriayri qurgulardan ibaretdir. yaxin gelecekde suni intellekte malik olan texnoloji varliqlarin coxalaraq insan cemiyyetine paralel olan texnocemiyyet yaradacagi proqnozlasdirilir. bele ki, son zamanlar v, vi senaye inqilablarindan behs edilir. yeni sosiotexnoloji cemiyyete kecidin bas vermesi ve bu hibrid, qarisiq cemiyyetde hemin intellektual texnoloji varliqlar ile insanlarin birge yasayisi, fealiyyet gostereceyi gozlenilir. bu da onlarin cemiyyetde ali meqama, dominant movqeye yukselmesi ile bagli riskleri artiracaqdir. suni intellekt cemiyyetde mueyyen funksiyalar dasidigi ucun onun da fealiyyetinin tenzimlenmesi, onu yaradanin qanun qarsisinda mesuliyyeti, cemiyyetde yeni intellektual texnologiyalardan istifade olunmasi ve hetta istifadeden sonra onlarin mehv olunmasi ile bagli mueyyen qaydaqanunlar, normativhuquqi senedler qebul olunmalidir. hazirda dunya olkeleri bas vere bilecek tehlukelerin qarsisinin alinmasi ucun muxtelif qerarlar, qanunlar qebul edirler. son zamanlar yuksek texnoloji inkisafin tesiri ile alimler artiq suni xususi intellektden suni umumi intellekte kecmek ucun ciddi arasdirmalar aparirlar. yeni meselen, alim suni intellekte her hansi bir canlinin ona ziyan vura bileceyini oyredir. lakin suni intellekt daha tehlukeli basqa bir canlini tanimadigi ucun ondan qorxmayacaq. bundan ferqli olaraq, insan bir canlinin tehlukeli oldugunu bildiyi halda onun beyninde mentiqi umumilesdirme prosesi gedir. bu da insanin mueyyen elametlerine, keyfiyyetlerine gore hemin sinfe daxil olan canlilardan qorxmali oldugunu da anlamasina imkan verir. bu baximdan tebii intellekt suni intellektden coxcox ustun hesab edilir. insanin induksiya esasinda, yeni xususiden umumiye, sadeden murekkebe dogru elametleri umumilesdirmesi tebii umumi intellekte aid xususiyyetdir. bunun da analoqu suni umumi intellektdir. bu gun movcud olan hec bir suni intellekt hele ki, insan kimi umumilesdirme qabiliyyetine malik deyil. ona gore de qavrama, oyrenme, obrazlari tanima, proqnozlasdirma, alternativ situasiyada qerar qebuletme ve s. kimi funksiyalar suni intellekte insan terefinden verilen konkret tapsiriqdan ireli gelen intellektual funksiyalardir. suni umumi intellektin arxasinda internet resurslari, aglasigmaz imkanlara malik olan superkomputer texnologiyalari, qabaqcil intellektual metodlar, modeller ve alqoritmler dayanir. hetta son zamanlar ifrat boyuk verilenlerin suretli emali ve oturulmesi problemleri ile elaqedar olaraq, kvant texnologiyalarina xususi diqqetin ayrilmasi zerureti yaranib. bele ki, artiq kvant komputerleri, kvant serverleri ve s. kimi kvant texnologiyalari informasiya partlayisi seraitinde cixis yollarindan biri kimi qeyd olunur. hazirda qabaqcil olkelerin tecrubesi kvant texnologiyalarinin boyuk strateji ehemiyyete malik oldugunu gosterir. bu baximdan, kvant texnologiyalarinin tetbiqi ve tekmillesdirilmesi inkisaf etmis olkelerin qarsisinda dayanan prioritet vezifelerden hesab edilir. belelikle, suni xususi intellektin suni umumi intellekte cevrilmesi ve bunun da esasinda dayanan elminezeri potensial, texnoloji inqilablar, yeni bir inkisaf merhelesi kimi suni super intellekt ideologiyasini aktuallasdirir. bu gun artiq iv senaye inqilabinin big data resurslarinin butun koordinatlar uzre ifrat suretle artimina dair cagirislari rehber tutularaq, suni super intellektin yaradilmasi istiqametinde muhum elmi tedqiqatlar aparilir, problemlerin helli ucun butun imkanlar seferber olunur.  indi bezi alimler superintellektin yaranmasi ehtimalindan behs edirler. bu barede ne deye bilersiniz  beli, artiq insanin bioloji, intellektual imkanlari ile silahlanan suni intellektin imkanlari gozlemediyimiz, tesevvur ede bilmeyeceyimiz hedde gelib cixmaqdadir. suni intellektin yaddas ve hesablama gucu, virtual platformada zaman ve mekandan asili olmayaraq fealiyyet gostermesi, cox muxtelif menbelerden boyuk hecmli melumatlar toplamasi, suretle analiz etmesi, onlardan yeni bilikler elde etmesi, proqnozlar vermesi ve s. kimi genis imkanlari, superintellektin yaranmasi perspektivleri onun gelecekde insani usteleye bilmesi ehtimali ile bagli fikirlerin ireli surulmesine esas verir, texnologiyalarin cemiyyetin aparici quvvesine cevrileceyi proqnozlasdirilir. bildiyiniz kimi, ferdin, ayriayri insanlarin tebii intellekti oldugu kimi, cemiyyetin de bir ictimai suuru, kollektiv tebii intellekti vardir. cemiyyet suretle inkisaf edir, insanlar yeni bilikler elde edirler. buna paralel olaraq, ayriayri suni intellekt qurgulari, proqram mehsullari ve s. ferdi suni intellekte malikdir. ferdi suni intellekt dasiyicilari, yeni hemin smart qurgular birbiri ile elaqe quracaq, ictimaileserek kollektiv suni intellekt yaradacaqlar. yaxin gelecekde ise kollektiv tebii intellekt ve kollektiv suni intellektin birge yasayisi, birbiri ile unsiyyet qurmasi, birbirine yararli olmasi, bezi hallarda ziyan vurmasi, mueyyen tehlukeler yaratmasi haqqinda bezen elmi, bezen futuroloji, bezen de fantastik mulahizeler ireli surulur. butun bunlarin neticesinde proqnozlasdirila bilmeyen mueyyen perspektivler ortaya cixa biler. internet muhitin getgede intellektuallasdirilmasi, hemin muhitde kollektiv suni intellektin yaranmasi artiq suni intellekt texnologiyalarinin inkisafinin novbeti merhelesi kimi, beser tarixinde ilk defe olaraq, yeni sosiotexnoloji cemiyyetin esasini formalasdirir. meselen, hemin intellektual texnologiyalar esasinda yaradilmis catbotlar ayriayri ferdi suni intellekt sistemlerinin uzerinde qurulan bir intellektual metasistemdir. bu metasistemi kollektiv suni intellektin formalasmasi kimi de qebul etmek olar. yeni insan cemiyyetinin ferdi tebii intellekti nece ictimailesir, inkisaf edirse, paralel olaraq, suni intellekt de suretle sosiallasir, inkisaf edir. basqa sozle, suni intellektin xususilikden, umumilikden super intellekt seviyyesine yukselmesi bir istiqametdirse, ikinci istiqamet ise onun basqa suni intellekt dasiyicilari olan qurgular, proqram mehsullari ile qarsiliqli elaqelendirilmesidir. bu zaman kutlevi sekilde birbirile elaqelendirilmis suni intellekt qurgulari kollektiv suni intellektin yaradilmasi ucun ciddi intellektual texnoloji platforma formalasdirir.  fealiyyeti tenzimlenen, yeni reqlamentlesdirilmis suni intellekt haqqinda ne deye bilersiniz  reqlamentlesdirilmis suni intellekt dedikde, insan terefinden fealiyyet cercivesi, parametrleri, telebleri verilen, tenzimlene bilen ve idare olunan suni intellekt nezerde tutulur. lakin suni intellekt ozozune oyrenme kimi bir xususiyyete malikdir ki, bu da onun mueyyen muddetden sonra qebul olunmus normalari, reqlamenti asmasina getirib cixara biler. yeri gelmisken, qeyd edim ki, hazirda suni intellektin mueyyen muddetden sonra ilkin reqlamentden kenara cixmasi, onun hansi seraitde nece qerar qebul etmesi de suni intellektin yaratdigi en boyuk tehlukelerden biridir. cunki suni intellekt texnologiyalari en murekkeb funksiyalari uzerine goturur ve bu funksiyalari heyata kecirerken suni neyron sebekeleri esasinda formalasan derin telim metodlarindan istifade edir. smart texnoloji muhitde generasiya olunan biliyin, qerarin ozunun elde olunmasinin izah oluna bilinmemesi kimi ciddi bir meqam suni intellekt sisteminin gelecekde ozunu nece aparacagini proqnozlasdirmaga imkan vermir ve bu sebebden de ciddi riskler yaranir. bele ki, suni intellektin qebul edeceyi qerarlar cemiyyet ucun tehlukeli ola, insanlarin saglamligina ve s. ciddi ziyan vura da biler. diger terefden, artiq kibertehlukelerin ozleri de bu smart texnologiyalardan istifade etmekle intellektuallasirlar. yeni butun bu intellektual texnologiyalar, eyni zamanda bedniyyetli insanlarin elinde cox guclu vasiteye, silaha cevrilir. ona gore de olkenin bu istiqametde informasiya tehlukesizliyinin temin olunmasi, kiberdayaniqliliginin daha da yukseldilmesi ucun aidiyyeti qurumlar, alimler ve mutexessisler intellektual potensialini seferber ederek kompleks isler heyata kecirirler.  suni intellektle elaqedar cemiyyeti dusunduren vacib suallardan biri de is yerlerinin azalmasi meselesi ile baglidir. siz nece dusunursunuz, bu istiqametde tehluke varmi  sozsuz ki, suni intellektin bezi isci quvvesini evez etmesi mueyyen tehlukeler yaradir. lakin bunu bilavasite suni intellektle, muasir texnologiyalarla elaqelendirmek duzgun deyil. cunki beser tarixinde insanlar her zaman oz varligini, ilkin heyat seraitini temin etmek, hemcinin mueyyen tehlukelerden qorunmaq, qarsilarina qoyduqlari muxtelif meqsed ve arzulara catmaq ucun heyvanlari ehlilesdirerek, onlardan neqliyyat, yukdasima vasitesi kimi ve diger meqsedler ucun istifade edibler. insanlar butun dovrlerde ozlerine komekci axtariblar, oz emeyini yungullesdirmeye, daha yaxsi yasamaga, qarsida dayanan problemleri aradan qaldirmaq ucun movcud imkanlardan istifade etmeye ve yeni imkanlar qazanmaga calisiblar. butun bunlar her dovrun realliqlarina ve inkisaf seviyyesine uygun emek bazarinin formalasmasina, yeni peselerin yaranmasina, eyni zamanda bezi peselerin siradan cixmasina sebeb olub. belelikle, bu qenaete gelmek olar ki, suni intellektin suretli inkisafi ile elaqedar olaraq is yerlerinin azalmasi ile beraber, yeni peselerin de meydana cixmasi tebii prosesdir. bugunku realliq ondan ibaretdir ki, insanlarin uzunmuddetli intellektual fealiyyetinin neticesi olaraq yeni texnologiyalarin meydana gelmesi ve insanlarin heyatinin terkib hissesine cevrilmesi emek bazarina ehemiyyetli derecede tesir edir ve onu deyisdirir. bunun neticesinde sivilizasiyanin yeni inkisaf merhelesine qedem qoyacagi proqnozlasdirilir. bu sebebden son zamanlar v ve vi senaye inqilablarinin cox da uzaqda olmadigindan behs edilir. muasir dovrde onlayn fealiyyet gostermek ucun cox sayda is yerleri, ixtisaslar yaranmaqdadir. mobil telefonlar, vebsaytlar, sosial sebekeler ve s. kimi platformalarda insanlar hetta bir nece is yerinde calismaq ucun boyuk imkanlar qazanirlar. basqa sozle, muasir dovrde cemiyyetin inkisafinin umumi tendensiyasi ondan ibaretdir ki, suni intellekt texnologiyalari teqdim etdikleri genis ustunlukler sayesinde en ali varliq olan insani adi, sade, rutin emeliyyatlari icra etmekden azad edir ve insanlari daha ali intellektual fealiyyetle mesgul olmaga sovq edir. hazirda reqemsal transformasiyalarin tesiri ile muxtelif peselerde ciddi deyisikliklerin bas vermesi neticesinde insanlardan yeni bilik ve bacariqlar teleb olunur. bu sebebden emek bazarinin xarakterinin deyismesi bir cox sahelerde, elece de kadr hazirligi istiqametinde yeni telebler ve vezifeler qoyur. elbette ki, muxtelif qurumlar, hemcinin insanlar anlayirlar ki, bu cagirislara cavab veren cemiyyetin vetendasi olmaq, aparilan islerde yaxindan istirak etmek, oz huquq ve vezifelerini heyata kecirmek ucun bu texnologiyalara derinden beled olmaq, onlarin inkisafi ile ayaqlasmaq lazimdir.  rasim muellim, azerbaycanda suni intellekt texnologiyalarinin meydana gelmesi ve inkisaf seviyyesi hansi veziyyetdedir  ilk olaraq, qeyd etmek isterdim ki, ci illerde  suni intellekt texnologiyalarinin tetbiq olunmaga basladigi ilk zamanlarda sovet hokumetinin terkibinde olan azerbaycanda da suni intellekt tesebbusleri ile bagli bir sira arasdirmalar aparilir, layiheler heyata kecirilirdi. hele ci illerde bu tedqiqatlara kibernetika institutunda, indiki idareetme sistemleri institutunda ve diger elmtehsil muessiselerinde de baslanilmisdi. hemin illerde azerbaycan elmler akademiyasinda suni intellektin texnoloji aspektleri ile yanasi, felsefi ve fizioloji problemleri ile bagli tedqiqatlar da aparilirdi. ci illerden sonra professor lutfi zadenin qeyriselis mentiq nezeriyyesinin olkemizde aparilan tedqiqatlara cox boyuk tesiri oldu. natamam informasiya, qeyrimueyyenlik seraitinde qerarlarin qebul edilmesi, qeyriselis qerar qebuletme, obrazlarin, tesvirlerin taninmasi, masin telimi, ekspert sistemlerinin yaradilmasi istiqametinde elminezeri arasdirmalara baslanildi ve bu arasdirmalarin neticelerinin neftqaz, kimya senayesinde ve diger sferalarda tetbiqi heyata kecirildi. bu istiqametde elde olunmus elminezeri ve praktiki nailiyyetlere gore bir qrup alimimiz ssri dovlet mukafatina layiq goruldu. musteqil azerbaycanin bugunku inkisaf merhelesinde ise dovlet bascisi ilham eliyev olkemizde suni intellekt texnologiyalarinin inkisafina boyuk diqqet ayirir. melum oldugu kimi, muxtelif olkelerde suni intellekt texnologiyalari ile bagli milli strategiyalar qebul olunur. bu tesebbuslere artiq olkemizde de baslanilib. hazirda bu islerin dovlet terefinden desteklenmesi, muvafiq qurumlara bununla bagli mueyyen tapsiriqlarin verilmesi, innovativ layihelere destek gosterilmesi istiqametinde zeruri tedbirler heyata kecirilir. azerbaycan alimleri de bu prosesde yaxindan istirak edirler. son zamanlar coxsayli suni intellekt problemleri, o cumleden agilli infrastrukturlarin qurulmasi ile bagli meselelerin tedqiq olunmasi azerbaycan alimleri qarsisinda vezife kimi qoyulub. azerbaycan milli elmler akademiyasinin, elm ve tehsil nazirliyinin muvafiq elmitedqiqat institutlari, ali tehsil muessiseleri suni intellekt meselelerine boyuk ehemiyyet verirler. bu istiqametde kadr hazirligi, tedris proqramlarina suni intellekt problemleri ile bagli yeni fenlerin elave olunmasi, muxtelif dovlet qurumlarinin fealiyyetinde, ayriayri infrastruktur layihelerinde bu texnologiyalarin genis tetbiq olunmasi isleri ugurla heyata kecirilmekdedir. qeyd etmek lazimdir ki, olke prezidentinin birbasa desteyi ile bu sahede yuksekixtisasli kadrlarin yetisdirilmesi ucun genclerimiz xaricde tehsil uzre dovlet proqrami cercivesinde dunyanin muxtelif olkelerinde yuksek tehsil almaq imkani qazanirlar. bir ehemiyyetli fakti nezere catdirmaq lazimdir ki, bu gun azerbaycan alimleri komputer elmleri sahesinde qafqazda birinci yerdedirler. alimlerimiz absin stenford universitetinin dunyanin en nufuzlu alimlerinin reytinq siyahisinda temsil olunurlar. sevindirici haldir ki, artiq olkemizde suni intellekt ideyalarinin ehemiyyetini, ustunluklerini her kes anlayir, genclerimiz bu texnologiyalara boyuk heves gosterirler. elm ve tehsil nazirliyi terefinden teskil olunan sabahin alimleri musabiqesi, sagirdlere reqemsal bilik ve bacariqlarin asilanmasi layihesi olan steam, bu yaxinlarda olkemizde kecirilmis teknofest aviasiya, kosmik ve texnologiya festivali kimi tesebbusler genclerin bu saheye maraginin artirilmasina boyuk destek verirler.\n",
      "\n",
      "Context: ['ki,', 'son', 'suni', 'intellekt']\n",
      "\n",
      "Prediction: zamanlar\n"
     ]
    }
   ],
   "source": [
    "context = ['ki,', 'son', 'suni', 'intellekt']\n",
    "for ix,word in enumerate(context):\n",
    "    context[ix] = unicodeToAscii(word)\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['daxil', 'olur.', 'texnologiyalar', 'dunya'], 'yeni')"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['daxil', 'olur.', 'texnologiyalar', 'dunya']\n",
      "\n",
      "Prediction: yeni\n"
     ]
    }
   ],
   "source": [
    "context = ['daxil', 'olur.', 'texnologiyalar', 'dunya']\n",
    "for ix,word in enumerate(context):\n",
    "    context[ix] = unicodeToAscii(word)\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['realliqlar', 'yaratmaqda,', 'ideologiyasina', 'cevrilmekdedir.'],\n",
       " 'dovrumuzun')"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['realliqlar', 'yaratmaqda,', 'ideologiyasina', 'cevrilmekdedir.']\n",
      "\n",
      "Prediction: dovrumuzun\n"
     ]
    }
   ],
   "source": [
    "context = ['realliqlar', 'yaratmaqda,', 'ideologiyasina', 'cevrilmekdedir.']\n",
    "for ix,word in enumerate(context):\n",
    "    context[ix] = unicodeToAscii(word)\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['merhelesinde', 'ise', 'bascisi', 'ilham'], 'dovlet')"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>usullari</td>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>ugurla</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>mutexessisler</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>cemiyyet,</td>\n",
       "      <td>0.000354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>azad</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>professor</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>dayaqlarindan</td>\n",
       "      <td>0.000561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>qurgular,</td>\n",
       "      <td>0.000638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>saglamligi</td>\n",
       "      <td>0.000688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>xas</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  similarity\n",
       "180       usullari    0.000199\n",
       "467         ugurla    0.000241\n",
       "604  mutexessisler    0.000292\n",
       "570      cemiyyet,    0.000354\n",
       "543           azad    0.000387\n",
       "575      professor    0.000459\n",
       "597  dayaqlarindan    0.000561\n",
       "975      qurgular,    0.000638\n",
       "106     saglamligi    0.000688\n",
       "738            xas    0.000724"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'dovlet'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})\n",
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eserlerinde', 'suni', 'meselelerine', 'toxunub,'], 'intellekt')"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>verilen</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>oyrenme</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>fundamental</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>ibaretdir.</td>\n",
       "      <td>0.000496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>mukafatina</td>\n",
       "      <td>0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>meqama,</td>\n",
       "      <td>0.000694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>cixarir.</td>\n",
       "      <td>0.000733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>institutlari,</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>etdikleri</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>ayriayri</td>\n",
       "      <td>0.001008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  similarity\n",
       "408         verilen    0.000013\n",
       "317         oyrenme    0.000430\n",
       "1198    fundamental    0.000449\n",
       "513      ibaretdir.    0.000496\n",
       "810      mukafatina    0.000553\n",
       "852         meqama,    0.000694\n",
       "1142       cixarir.    0.000733\n",
       "900   institutlari,    0.000800\n",
       "816       etdikleri    0.000818\n",
       "879        ayriayri    0.001008"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'suni'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})\n",
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['maraginin', 'artirilmasina', 'destek', 'verirler.'], 'boyuk')"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>behs</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>lutfi</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>inqilablarindan</td>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>isler</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>malikdir.</td>\n",
       "      <td>0.000649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>isteyirem.</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>gelecekle</td>\n",
       "      <td>0.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>murekkeb</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>intellekt</td>\n",
       "      <td>0.000958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>yeni</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  similarity\n",
       "510              behs    0.000185\n",
       "910             lutfi    0.000329\n",
       "1114  inqilablarindan    0.000575\n",
       "964             isler    0.000633\n",
       "1017        malikdir.    0.000649\n",
       "988        isteyirem.    0.000747\n",
       "149         gelecekle    0.000765\n",
       "1189         murekkeb    0.000901\n",
       "229         intellekt    0.000958\n",
       "350              yeni    0.001220"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'destek'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})\n",
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>meqalesi</td>\n",
       "      <td>0.000319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>canlilarin</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>genetik</td>\n",
       "      <td>0.000526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>fantastik</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>aglasigmaz</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>induksiya</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>yox</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>telebleri</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>silahlar</td>\n",
       "      <td>0.001036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>zade</td>\n",
       "      <td>0.001095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  similarity\n",
       "902     meqalesi    0.000319\n",
       "579   canlilarin    0.000487\n",
       "328      genetik    0.000526\n",
       "389    fantastik    0.000589\n",
       "1205  aglasigmaz    0.000592\n",
       "958    induksiya    0.000637\n",
       "397          yox    0.000768\n",
       "987    telebleri    0.000897\n",
       "945     silahlar    0.001036\n",
       "105         zade    0.001095"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'boyuk'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})\n",
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['asilanmasi', 'layihesi', 'steam,', 'bu'], 'olan')"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>proqrami</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>imkanlarla</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>silah</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>mualice</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>hedde</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>ise</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>dasidigi</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>alimlerimiz</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>olmusdur.</td>\n",
       "      <td>0.000579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>asanliqla</td>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  similarity\n",
       "335      proqrami    0.000064\n",
       "351    imkanlarla    0.000115\n",
       "1006        silah    0.000232\n",
       "1023      mualice    0.000468\n",
       "1122        hedde    0.000480\n",
       "673           ise    0.000505\n",
       "505      dasidigi    0.000542\n",
       "1210  alimlerimiz    0.000552\n",
       "462     olmusdur.    0.000579\n",
       "798     asanliqla    0.000626"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'steam,'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})\n",
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['olunan', 'sabahin', 'musabiqesi,', 'sagirdlere'], 'alimleri')"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>peselerin</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>riskler</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>arasdirmalar</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>texnologiyalarla</td>\n",
       "      <td>0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>teskil</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>birbiri</td>\n",
       "      <td>0.000322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>imkanlardan</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>yaxsi</td>\n",
       "      <td>0.000431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>basqa,</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>maraginin</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  similarity\n",
       "759          peselerin    0.000040\n",
       "598            riskler    0.000103\n",
       "506       arasdirmalar    0.000140\n",
       "527   texnologiyalarla    0.000186\n",
       "1196            teskil    0.000253\n",
       "366            birbiri    0.000322\n",
       "828        imkanlardan    0.000334\n",
       "276              yaxsi    0.000431\n",
       "217             basqa,    0.000655\n",
       "214          maraginin    0.000668"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'sabahin'\n",
    "word_arr = []\n",
    "dist_arr = []\n",
    "for w in vocab:\n",
    "    dist = torch.cosine_similarity(model.embeddings.weight[word_to_ix[word]].unsqueeze(0),model.embeddings.weight[word_to_ix[w]].unsqueeze(0))\n",
    "    word_arr.append(w)\n",
    "    dist_arr.append(torch.norm(dist).item())\n",
    "df = pd.DataFrame({'word':word_arr,'similarity':dist_arr})\n",
    "df.sort_values(by='similarity').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
